{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de02212b-9153-4042-a695-6683fd75a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# WEEK 13\n",
    "# Final Notebook/Report Program (Hotel Bookings Dataset)\n",
    "# Dataset: https://raw.githubusercontent.com/swapnilsaurav/Dataset/refs/heads/master/hotel_bookings.csv\n",
    "# Target: is_canceled (0 = not canceled, 1 = canceled)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9169643-a36d-4b13-92d7-f21769768260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shape: (119390, 33)\n",
      "   id         hotel  is_canceled  lead_time  arrival_date_year  \\\n",
      "0   1  Resort Hotel          0.0      342.0               2015   \n",
      "1   2  Resort Hotel          0.0      737.0               2015   \n",
      "2   3  Resort Hotel          0.0        7.0               2015   \n",
      "\n",
      "  arrival_date_month  arrival_date_week_number  arrival_date_day_of_month  \\\n",
      "0               July                      27.0                        1.0   \n",
      "1               July                      27.0                        1.0   \n",
      "2               July                      27.0                        1.0   \n",
      "\n",
      "   stays_in_weekend_nights  stays_in_week_nights  ...  deposit_type  agent  \\\n",
      "0                      0.0                   0.0  ...    No Deposit    NaN   \n",
      "1                      0.0                   0.0  ...    No Deposit    NaN   \n",
      "2                      0.0                   1.0  ...    No Deposit    NaN   \n",
      "\n",
      "   company days_in_waiting_list customer_type   adr  \\\n",
      "0      NaN                  0.0     Transient   0.0   \n",
      "1      NaN                  0.0     Transient   0.0   \n",
      "2      NaN                  0.0     Transient  75.0   \n",
      "\n",
      "  required_car_parking_spaces  total_of_special_requests  reservation_status  \\\n",
      "0                         0.0                        0.0           Check-Out   \n",
      "1                         0.0                       10.0           Check-Out   \n",
      "2                         0.0                        0.0           Check-Out   \n",
      "\n",
      "   reservation_status_date  \n",
      "0               01-07-2015  \n",
      "1               01-07-2015  \n",
      "2               02-07-2015  \n",
      "\n",
      "[3 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1) Problem Statement & KPIs\n",
    "# -----------------------------\n",
    "\"\"\"\n",
    "Business Objective:\n",
    "Predict booking cancellation (is_canceled) using stable, clean, meaningful features.\n",
    "\n",
    "Preprocessing KPIs (data-ready success):\n",
    "- Missing values handled with defensible strategies\n",
    "- Categorical consistency ensured (unknowns handled)\n",
    "- Numeric scaling applied properly (after split, on train only)\n",
    "- High-dimensional one-hot compressed using SVD for stability and speed\n",
    "- Anomalies detected and flagged (not blindly removed)\n",
    "- Pipeline is reproducible and deployable (saved artifacts)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Load Dataset\n",
    "# -----------------------------\n",
    "DATA_URL = \"https://raw.githubusercontent.com/swapnilsaurav/Dataset/refs/heads/master/hotel_bookings.csv\"\n",
    "df = pd.read_csv(DATA_URL)\n",
    "\n",
    "print(\"Raw shape:\", df.shape)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4980573d-1e33-4a3f-ae7c-9584c97f8fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values (top 15):\n",
      "company                      112593\n",
      "agent                         16353\n",
      "meal                          13703\n",
      "babies                        13517\n",
      "deposit_type                   9842\n",
      "children                       2461\n",
      "country                         502\n",
      "assigned_room_type               14\n",
      "arrival_date_day_of_month        14\n",
      "market_segment                   14\n",
      "adults                           14\n",
      "distribution_channel             14\n",
      "reserved_room_type               14\n",
      "is_repeated_guest                14\n",
      "stays_in_weekend_nights          14\n",
      "dtype: int64\n",
      "\n",
      "Target distribution:\n",
      "is_canceled\n",
      "0.0    75156\n",
      "1.0    44224\n",
      "NaN       10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 3) Basic Sanity Checks\n",
    "# -----------------------------\n",
    "if \"is_canceled\" not in df.columns:\n",
    "    raise ValueError(\"Target column 'is_canceled' not found in dataset.\")\n",
    "\n",
    "print(\"\\nMissing values (top 15):\")\n",
    "print(df.isna().sum().sort_values(ascending=False).head(15))\n",
    "\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df[\"is_canceled\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a79f78b7-4b42-4aca-82b0-5fd1adbe8c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After feature engineering shape: (119390, 38)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4) Feature Engineering (Hotel-Specific)\n",
    "#    Keep it simple, meaningful, explainable.\n",
    "# -----------------------------\n",
    "def feature_engineering(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = data.copy()\n",
    "\n",
    "    # Ensure typical missing numeric values are filled before derived features\n",
    "    for col in [\"children\", \"babies\", \"adults\", \"stays_in_weekend_nights\", \"stays_in_week_nights\"]:\n",
    "        if col in d.columns:\n",
    "            d[col] = d[col].fillna(0)\n",
    "\n",
    "    # Total nights\n",
    "    if \"stays_in_weekend_nights\" in d.columns and \"stays_in_week_nights\" in d.columns:\n",
    "        d[\"total_nights\"] = d[\"stays_in_weekend_nights\"] + d[\"stays_in_week_nights\"]\n",
    "    else:\n",
    "        d[\"total_nights\"] = np.nan\n",
    "\n",
    "    # Total guests\n",
    "    if set([\"adults\", \"children\", \"babies\"]).issubset(d.columns):\n",
    "        d[\"total_guests\"] = d[\"adults\"] + d[\"children\"] + d[\"babies\"]\n",
    "    else:\n",
    "        d[\"total_guests\"] = np.nan\n",
    "\n",
    "    # Family flag\n",
    "    if set([\"children\", \"babies\"]).issubset(d.columns):\n",
    "        d[\"is_family\"] = ((d[\"children\"] + d[\"babies\"]) > 0).astype(int)\n",
    "    else:\n",
    "        d[\"is_family\"] = 0\n",
    "\n",
    "    # ADR per person (avoid divide by zero)\n",
    "    if \"adr\" in d.columns and \"total_guests\" in d.columns:\n",
    "        denom = d[\"total_guests\"].replace(0, np.nan)\n",
    "        d[\"adr_per_person\"] = d[\"adr\"] / denom\n",
    "    else:\n",
    "        d[\"adr_per_person\"] = np.nan\n",
    "\n",
    "    # Lead time log transform (reduce skew)\n",
    "    if \"lead_time\" in d.columns:\n",
    "        d[\"lead_time_log1p\"] = np.log1p(d[\"lead_time\"].clip(lower=0))\n",
    "    else:\n",
    "        d[\"lead_time_log1p\"] = np.nan\n",
    "\n",
    "    # Month mapping (arrival_date_month is usually text)\n",
    "    # Convert month name -> month number for better modeling\n",
    "    if \"arrival_date_month\" in d.columns:\n",
    "        month_map = {\n",
    "            \"January\": 1, \"February\": 2, \"March\": 3, \"April\": 4,\n",
    "            \"May\": 5, \"June\": 6, \"July\": 7, \"August\": 8,\n",
    "            \"September\": 9, \"October\": 10, \"November\": 11, \"December\": 12\n",
    "        }\n",
    "        d[\"arrival_month_num\"] = d[\"arrival_date_month\"].map(month_map)\n",
    "    else:\n",
    "        d[\"arrival_month_num\"] = np.nan\n",
    "\n",
    "    # Booking intensity: special requests per night (proxy for intent/complexity)\n",
    "    if \"total_of_special_requests\" in d.columns and \"total_nights\" in d.columns:\n",
    "        denom = d[\"total_nights\"].replace(0, np.nan)\n",
    "        d[\"requests_per_night\"] = d[\"total_of_special_requests\"] / denom\n",
    "    else:\n",
    "        d[\"requests_per_night\"] = np.nan\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "df_fe = feature_engineering(df)\n",
    "\n",
    "# (Optional) Drop columns that can cause leakage or are post-outcome.\n",
    "# Example: reservation_status_date could contain future-ish timing relative to label creation.\n",
    "# We'll drop date string columns and obvious post-decision fields if present.\n",
    "LEAKY_OR_LOW_VALUE = [\n",
    "    \"reservation_status_date\",   # date string; can leak timeline info\n",
    "    \"reservation_status\"         # status often correlated with cancellation outcome\n",
    "]\n",
    "for c in LEAKY_OR_LOW_VALUE:\n",
    "    if c in df_fe.columns:\n",
    "        df_fe = df_fe.drop(columns=[c])\n",
    "\n",
    "print(\"\\nAfter feature engineering shape:\", df_fe.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45994b24-c9a7-4143-a947-5da5efda58bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "IntCastingNaNError",
     "evalue": "Cannot convert non-finite values (NA or inf) to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIntCastingNaNError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 5) Train/Test Split (No Leakage)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m X = df_fe.drop(columns=[\u001b[33m\"\u001b[39m\u001b[33mis_canceled\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m y = \u001b[43mdf_fe\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mis_canceled\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[32m      8\u001b[39m     X, y,\n\u001b[32m      9\u001b[39m     test_size=\u001b[32m0.2\u001b[39m,\n\u001b[32m     10\u001b[39m     random_state=RANDOM_STATE,\n\u001b[32m     11\u001b[39m     stratify=y\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTrain shape:\u001b[39m\u001b[33m\"\u001b[39m, X_train.shape, \u001b[33m\"\u001b[39m\u001b[33m| Test shape:\u001b[39m\u001b[33m\"\u001b[39m, X_test.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:6665\u001b[39m, in \u001b[36mNDFrame.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m   6659\u001b[39m     results = [\n\u001b[32m   6660\u001b[39m         ser.astype(dtype, copy=copy, errors=errors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()\n\u001b[32m   6661\u001b[39m     ]\n\u001b[32m   6663\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6664\u001b[39m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6665\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6666\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes)\n\u001b[32m   6667\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mastype\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:449\u001b[39m, in \u001b[36mBaseBlockManager.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[32m    447\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mastype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:784\u001b[39m, in \u001b[36mBlock.astype\u001b[39m\u001b[34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[39m\n\u001b[32m    781\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not squeeze with more than one column.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    782\u001b[39m     values = values[\u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m new_values = \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m new_values = maybe_coerce_values(new_values)\n\u001b[32m    788\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[39m, in \u001b[36mastype_array_safe\u001b[39m\u001b[34m(values, dtype, copy, errors)\u001b[39m\n\u001b[32m    234\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     new_values = \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    179\u001b[39m     values = values.astype(dtype, copy=copy)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     values = \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:101\u001b[39m, in \u001b[36m_astype_nansafe\u001b[39m\u001b[34m(arr, dtype, copy, skipna)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.ensure_string_array(\n\u001b[32m     97\u001b[39m         arr, skipna=skipna, convert_na_value=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     98\u001b[39m     ).reshape(shape)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m np.issubdtype(arr.dtype, np.floating) \u001b[38;5;129;01mand\u001b[39;00m dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33miu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_astype_float_to_int_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arr.dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# if we have a datetime/timedelta array of objects\u001b[39;00m\n\u001b[32m    105\u001b[39m     \u001b[38;5;66;03m# then coerce to datetime64[ns] and use DatetimeArray.astype\u001b[39;00m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lib.is_np_dtype(dtype, \u001b[33m\"\u001b[39m\u001b[33mM\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:145\u001b[39m, in \u001b[36m_astype_float_to_int_nansafe\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[33;03mastype with a check preventing converting NaN to an meaningless integer value.\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(values).all():\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IntCastingNaNError(\n\u001b[32m    146\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot convert non-finite values (NA or inf) to integer\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    147\u001b[39m     )\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype.kind == \u001b[33m\"\u001b[39m\u001b[33mu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;66;03m# GH#45151\u001b[39;00m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (values >= \u001b[32m0\u001b[39m).all():\n",
      "\u001b[31mIntCastingNaNError\u001b[39m: Cannot convert non-finite values (NA or inf) to integer"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5) Train/Test Split (No Leakage)\n",
    "# -----------------------------\n",
    "X = df_fe.drop(columns=[\"is_canceled\"])\n",
    "y = df_fe[\"is_canceled\"].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTrain shape:\", X_train.shape, \"| Test shape:\", X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f10f6132-4316-435f-9dad-5f90eeb999df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 6) Identify Column Types\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#    Note: object columns treated as categorical.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m numeric_features = \u001b[43mX_train\u001b[49m.select_dtypes(include=[\u001b[33m\"\u001b[39m\u001b[33mint64\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfloat64\u001b[39m\u001b[33m\"\u001b[39m]).columns.tolist()\n\u001b[32m      6\u001b[39m categorical_features = X_train.select_dtypes(include=[\u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m]).columns.tolist()\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNumeric features:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(numeric_features))\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6) Identify Column Types\n",
    "#    Note: object columns treated as categorical.\n",
    "# -----------------------------\n",
    "numeric_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(\"\\nNumeric features:\", len(numeric_features))\n",
    "print(\"Categorical features:\", len(categorical_features))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Preprocessing Pipelines\n",
    "# -----------------------------\n",
    "# Numeric: median impute + standard scale\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical: most_frequent impute + one-hot\n",
    "# Use sparse output to keep memory manageable.\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipeline, numeric_features),\n",
    "        (\"cat\", categorical_pipeline, categorical_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afe4547b-a5c7-4ed0-968a-2d5292b19a11",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 8) Fit/Transform\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m X_train_prep = \u001b[43mpreprocessor\u001b[49m.fit_transform(X_train)\n\u001b[32m      5\u001b[39m X_test_prep = preprocessor.transform(X_test)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAfter preprocessing:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 8) Fit/Transform\n",
    "# -----------------------------\n",
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "X_test_prep = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"\\nAfter preprocessing:\")\n",
    "print(\"Train matrix shape:\", X_train_prep.shape)\n",
    "print(\"Test matrix shape:\", X_test_prep.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2807fa6-0dcb-495b-8fc5-7082abb9a995",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_prep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 9) Feature Filtering: Remove Near-Zero Variance\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#    Works with sparse matrices.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      5\u001b[39m var_filter = VarianceThreshold(threshold=\u001b[32m0.0001\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m X_train_var = var_filter.fit_transform(\u001b[43mX_train_prep\u001b[49m)\n\u001b[32m      7\u001b[39m X_test_var = var_filter.transform(X_test_prep)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAfter VarianceThreshold:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_prep' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9) Feature Filtering: Remove Near-Zero Variance\n",
    "#    Works with sparse matrices.\n",
    "# -----------------------------\n",
    "var_filter = VarianceThreshold(threshold=0.0001)\n",
    "X_train_var = var_filter.fit_transform(X_train_prep)\n",
    "X_test_var = var_filter.transform(X_test_prep)\n",
    "\n",
    "print(\"\\nAfter VarianceThreshold:\")\n",
    "print(\"Train matrix shape:\", X_train_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62ae1e37-b856-4ef9-a237-346608ac87a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_var' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m N_COMPONENTS = \u001b[32m80\u001b[39m\n\u001b[32m      9\u001b[39m svd = TruncatedSVD(n_components=N_COMPONENTS, random_state=RANDOM_STATE)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m X_train_svd = svd.fit_transform(\u001b[43mX_train_var\u001b[49m)\n\u001b[32m     11\u001b[39m X_test_svd = svd.transform(X_test_var)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAfter TruncatedSVD:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_var' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 10) Dimensionality Reduction\n",
    "#     For one-hot sparse data, use TruncatedSVD (SVD is appropriate; PCA isn't ideal here)\n",
    "# -----------------------------\n",
    "# Choose components based on size; keep it practical for reporting.\n",
    "# Rule of thumb: 50â€“200 for demos; increase for larger modeling needs.\n",
    "N_COMPONENTS = 80\n",
    "\n",
    "svd = TruncatedSVD(n_components=N_COMPONENTS, random_state=RANDOM_STATE)\n",
    "X_train_svd = svd.fit_transform(X_train_var)\n",
    "X_test_svd = svd.transform(X_test_var)\n",
    "\n",
    "print(\"\\nAfter TruncatedSVD:\")\n",
    "print(\"Train reduced shape:\", X_train_svd.shape)\n",
    "print(\"Explained variance ratio (approx):\", float(np.sum(svd.explained_variance_ratio_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bae697c-4ac3-4657-bb42-b8bb4ac9a0b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_svd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 11) Anomaly Detection\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#     Strategy: Flag anomalies instead of removing.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# LOF: only fit_predict on train (no separate predict API reliably for novelty=False)\u001b[39;00m\n\u001b[32m      6\u001b[39m lof = LocalOutlierFactor(n_neighbors=\u001b[32m25\u001b[39m, contamination=\u001b[32m0.03\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m lof_train_labels = lof.fit_predict(\u001b[43mX_train_svd\u001b[49m)  \u001b[38;5;66;03m# -1 anomaly, 1 normal\u001b[39;00m\n\u001b[32m      8\u001b[39m lof_train_flag = (lof_train_labels == -\u001b[32m1\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Isolation Forest: can predict train + test\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_svd' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 11) Anomaly Detection\n",
    "#     Strategy: Flag anomalies instead of removing.\n",
    "# -----------------------------\n",
    "# LOF: only fit_predict on train (no separate predict API reliably for novelty=False)\n",
    "lof = LocalOutlierFactor(n_neighbors=25, contamination=0.03)\n",
    "lof_train_labels = lof.fit_predict(X_train_svd)  # -1 anomaly, 1 normal\n",
    "lof_train_flag = (lof_train_labels == -1).astype(int)\n",
    "\n",
    "# Isolation Forest: can predict train + test\n",
    "iso = IsolationForest(\n",
    "    n_estimators=300,\n",
    "    contamination=0.03,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "iso.fit(X_train_svd)\n",
    "\n",
    "iso_train_labels = iso.predict(X_train_svd)  # -1 anomaly, 1 normal\n",
    "iso_test_labels = iso.predict(X_test_svd)\n",
    "\n",
    "iso_train_flag = (iso_train_labels == -1).astype(int)\n",
    "iso_test_flag = (iso_test_labels == -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "532e7e02-03ed-4fe8-b760-503cedaa728d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_svd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 12) Final Feature Set\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#     Add anomaly flags as final features\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m X_train_final = np.column_stack([\u001b[43mX_train_svd\u001b[49m, lof_train_flag, iso_train_flag])\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# LOF flag not available on test (by design). Use -1 sentinel or 0.\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Here we put a neutral 0 for LOF test flag to keep schema consistent.\u001b[39;00m\n\u001b[32m      9\u001b[39m lof_test_flag = np.zeros(shape=(X_test_svd.shape[\u001b[32m0\u001b[39m],), dtype=\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_svd' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 12) Final Feature Set\n",
    "#     Add anomaly flags as final features\n",
    "# -----------------------------\n",
    "X_train_final = np.column_stack([X_train_svd, lof_train_flag, iso_train_flag])\n",
    "\n",
    "# LOF flag not available on test (by design). Use -1 sentinel or 0.\n",
    "# Here we put a neutral 0 for LOF test flag to keep schema consistent.\n",
    "lof_test_flag = np.zeros(shape=(X_test_svd.shape[0],), dtype=int)\n",
    "X_test_final = np.column_stack([X_test_svd, lof_test_flag, iso_test_flag])\n",
    "\n",
    "print(\"\\nFinal Train shape:\", X_train_final.shape)\n",
    "print(\"Final Test shape:\", X_test_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d2909a4-4ac8-4c1e-b7fd-708c06139032",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_prep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 13) Preprocessing Impact Summary (Report-Ready)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m impact_summary = {\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRaw feature columns (X)\u001b[39m\u001b[33m\"\u001b[39m: X.shape[\u001b[32m1\u001b[39m],\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAfter preprocessing (encoded)\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mX_train_prep\u001b[49m.shape[\u001b[32m1\u001b[39m],\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAfter variance filtering\u001b[39m\u001b[33m\"\u001b[39m: X_train_var.shape[\u001b[32m1\u001b[39m],\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAfter SVD (compressed)\u001b[39m\u001b[33m\"\u001b[39m: X_train_svd.shape[\u001b[32m1\u001b[39m],\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinal features (+anomaly flags)\u001b[39m\u001b[33m\"\u001b[39m: X_train_final.shape[\u001b[32m1\u001b[39m],\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mApprox explained variance (SVD)\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np.sum(svd.explained_variance_ratio_)),\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTrain anomaly rate (LOF)\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np.mean(lof_train_flag)),\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTrain anomaly rate (IsoForest)\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np.mean(iso_train_flag)),\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTest anomaly rate (IsoForest)\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np.mean(iso_test_flag))\n\u001b[32m     14\u001b[39m }\n\u001b[32m     16\u001b[39m impact_df = pd.DataFrame.from_dict(impact_summary, orient=\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m, columns=[\u001b[33m\"\u001b[39m\u001b[33mValue\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== PREPROCESSING IMPACT SUMMARY ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_prep' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 13) Preprocessing Impact Summary (Report-Ready)\n",
    "# -----------------------------\n",
    "impact_summary = {\n",
    "    \"Raw feature columns (X)\": X.shape[1],\n",
    "    \"After preprocessing (encoded)\": X_train_prep.shape[1],\n",
    "    \"After variance filtering\": X_train_var.shape[1],\n",
    "    \"After SVD (compressed)\": X_train_svd.shape[1],\n",
    "    \"Final features (+anomaly flags)\": X_train_final.shape[1],\n",
    "    \"Approx explained variance (SVD)\": float(np.sum(svd.explained_variance_ratio_)),\n",
    "    \"Train anomaly rate (LOF)\": float(np.mean(lof_train_flag)),\n",
    "    \"Train anomaly rate (IsoForest)\": float(np.mean(iso_train_flag)),\n",
    "    \"Test anomaly rate (IsoForest)\": float(np.mean(iso_test_flag))\n",
    "}\n",
    "\n",
    "impact_df = pd.DataFrame.from_dict(impact_summary, orient=\"index\", columns=[\"Value\"])\n",
    "print(\"\\n=== PREPROCESSING IMPACT SUMMARY ===\")\n",
    "print(impact_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85e0db69-15d9-4fd5-8784-2229b8226010",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TruncatedSVD' object has no attribute 'explained_variance_ratio_'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 14) Simple Visual Checks (Report-Friendly)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# A) SVD explained variance contribution (first 30 components)\u001b[39;00m\n\u001b[32m      5\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m plt.plot(np.cumsum(\u001b[43msvd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexplained_variance_ratio_\u001b[49m[:\u001b[32m30\u001b[39m]))\n\u001b[32m      7\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mComponents (first 30)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m plt.ylabel(\u001b[33m\"\u001b[39m\u001b[33mCumulative explained variance (approx)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'TruncatedSVD' object has no attribute 'explained_variance_ratio_'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 14) Simple Visual Checks (Report-Friendly)\n",
    "# -----------------------------\n",
    "# A) SVD explained variance contribution (first 30 components)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.cumsum(svd.explained_variance_ratio_[:30]))\n",
    "plt.xlabel(\"Components (first 30)\")\n",
    "plt.ylabel(\"Cumulative explained variance (approx)\")\n",
    "plt.title(\"SVD: Cumulative Explained Variance (First 30 Components)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# B) Anomaly rates (train)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"LOF(train)\", \"IsoForest(train)\", \"IsoForest(test)\"],\n",
    "        [np.mean(lof_train_flag), np.mean(iso_train_flag), np.mean(iso_test_flag)])\n",
    "plt.ylabel(\"Anomaly rate\")\n",
    "plt.title(\"Anomaly Flag Rates\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd5e1476-0a36-4429-9187-f831bcb4627d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 15) Save Artifacts for Deployment\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#     Save only what you need to reproduce transformations.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      5\u001b[39m ARTIFACTS_PATH = \u001b[33m\"\u001b[39m\u001b[33mhotel_preprocessing_artifacts.joblib\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m artifacts = {\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpreprocessor\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mpreprocessor\u001b[49m,\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvariance_filter\u001b[39m\u001b[33m\"\u001b[39m: var_filter,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msvd\u001b[39m\u001b[33m\"\u001b[39m: svd,\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33miso_forest\u001b[39m\u001b[33m\"\u001b[39m: iso,\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnumeric_features\u001b[39m\u001b[33m\"\u001b[39m: numeric_features,\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcategorical_features\u001b[39m\u001b[33m\"\u001b[39m: categorical_features,\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mN_COMPONENTS\u001b[39m\u001b[33m\"\u001b[39m: N_COMPONENTS,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrandom_state\u001b[39m\u001b[33m\"\u001b[39m: RANDOM_STATE\n\u001b[32m     16\u001b[39m }\n\u001b[32m     18\u001b[39m joblib.dump(artifacts, ARTIFACTS_PATH)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mArtifacts saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mARTIFACTS_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 15) Save Artifacts for Deployment\n",
    "#     Save only what you need to reproduce transformations.\n",
    "# -----------------------------\n",
    "ARTIFACTS_PATH = \"hotel_preprocessing_artifacts.joblib\"\n",
    "\n",
    "artifacts = {\n",
    "    \"preprocessor\": preprocessor,\n",
    "    \"variance_filter\": var_filter,\n",
    "    \"svd\": svd,\n",
    "    \"iso_forest\": iso,\n",
    "    \"numeric_features\": numeric_features,\n",
    "    \"categorical_features\": categorical_features,\n",
    "    \"N_COMPONENTS\": N_COMPONENTS,\n",
    "    \"random_state\": RANDOM_STATE\n",
    "}\n",
    "\n",
    "joblib.dump(artifacts, ARTIFACTS_PATH)\n",
    "print(f\"\\nArtifacts saved to: {ARTIFACTS_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50fb66c5-6ef5-4aad-84ce-954bf7257269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline is ready for notebook/report submission and deployment reuse.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 16) (Optional) Helper: Transform New Data for Production\n",
    "# -----------------------------\n",
    "def transform_new_data(raw_df: pd.DataFrame, artifacts_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Transform raw input data into the final feature matrix used for modeling.\n",
    "    This mirrors the training preprocessing pipeline.\n",
    "\n",
    "    Note:\n",
    "    - LOF flag for new data is not computed here (LOF is train-only in this setup),\n",
    "      so we set LOF flag = 0 for new rows.\n",
    "    - IsoForest flag is computed and appended.\n",
    "    \"\"\"\n",
    "    artifacts_local = joblib.load(artifacts_path)\n",
    "\n",
    "    preproc = artifacts_local[\"preprocessor\"]\n",
    "    vfilter = artifacts_local[\"variance_filter\"]\n",
    "    svd_local = artifacts_local[\"svd\"]\n",
    "    iso_local = artifacts_local[\"iso_forest\"]\n",
    "\n",
    "    # apply same feature engineering and same column drops\n",
    "    d = feature_engineering(raw_df)\n",
    "    for c in LEAKY_OR_LOW_VALUE:\n",
    "        if c in d.columns:\n",
    "            d = d.drop(columns=[c])\n",
    "\n",
    "    Xp = preproc.transform(d)\n",
    "    Xv = vfilter.transform(Xp)\n",
    "    Xr = svd_local.transform(Xv)\n",
    "\n",
    "    iso_flag = (iso_local.predict(Xr) == -1).astype(int)\n",
    "    lof_flag = np.zeros((Xr.shape[0],), dtype=int)\n",
    "\n",
    "    X_final = np.column_stack([Xr, lof_flag, iso_flag])\n",
    "    return X_final\n",
    "\n",
    "print(\"\\nPipeline is ready for notebook/report submission and deployment reuse.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e2a88-ed7c-4ea2-86b4-4d9c2fd0328f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
