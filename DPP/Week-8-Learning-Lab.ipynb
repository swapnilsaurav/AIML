{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d4d6de-4252-4ca4-b5e8-bc434ab97dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: 20\n",
      "Categorical features: 12\n",
      "Rows: 119380\n",
      "\n",
      "BASELINE: {'Stage': 'Baseline (All features)', 'Accuracy': 1.0, 'AUC': 1.0, 'Fit_time_sec': 2.8037643432617188}\n",
      "\n",
      "FILTER VAR: {'Stage': 'Filter: VarianceThreshold(0.01)', 'Accuracy': 1.0, 'AUC': 1.0, 'Fit_time_sec': 3.5674855709075928}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "WEEK 8 DEMO (Hotel Bookings Dataset): Filter + Wrapper + Embedded Feature Selection\n",
    "Dataset: hotel_bookings.csv (GitHub raw)\n",
    "\n",
    "Concepts covered this week:\n",
    "1) Filter methods: VarianceThreshold, Mutual Information (MI)\n",
    "2) Wrapper methods: Sequential Forward Selection (SFS), RFE\n",
    "3) Embedded methods: L1 Logistic Regression (LASSO-style), Tree-based feature importance\n",
    "4) Hybrid pipeline: Filter -> Wrapper -> Embedded\n",
    "5) Compare accuracy/AUC + runtime + selected feature counts\n",
    "\n",
    "Target used: 'is_canceled' (0/1)\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_selection import (\n",
    "    VarianceThreshold,\n",
    "    SelectKBest,\n",
    "    mutual_info_classif,\n",
    "    SequentialFeatureSelector,\n",
    "    RFE\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load dataset\n",
    "# -----------------------------\n",
    "url = \"https://raw.githubusercontent.com/swapnilsaurav/Dataset/refs/heads/master/hotel_bookings.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Basic cleaning: drop duplicates if any (optional)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Target\n",
    "target = \"is_canceled\"\n",
    "if target not in df.columns:\n",
    "    raise ValueError(f\"Target column '{target}' not found. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "#contains missing values ?\n",
    "df[target].isna().sum()\n",
    "# you cannot train a supervised model without a valid label.\n",
    "df = df.dropna(subset=[target])\n",
    "\n",
    "y = df[target].astype(int)\n",
    "X = df.drop(columns=[target])\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Identify numeric & categorical columns\n",
    "# -----------------------------\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=[\"object\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# If dataset has date-like strings that are objects, keep them categorical for this demo.\n",
    "print(\"Numeric features:\", len(numeric_features))\n",
    "print(\"Categorical features:\", len(categorical_features))\n",
    "print(\"Rows:\", len(df))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Train/Test split\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Preprocessor (impute + encode + scale)\n",
    "#    - We create a \"model-ready matrix\" from raw dataframe.\n",
    "# -----------------------------\n",
    "numeric_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, numeric_features),\n",
    "        (\"cat\", categorical_pipe, categorical_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Utility: feature names after preprocessing\n",
    "# -----------------------------\n",
    "def get_feature_names(preprocessor, numeric_features, categorical_features):\n",
    "    \"\"\"\n",
    "    Extract output feature names after ColumnTransformer.\n",
    "    Works for OneHotEncoder and numeric passthrough.\n",
    "    \"\"\"\n",
    "    # Fit a clone-like? We'll use the already-fitted preprocessor.\n",
    "    num_names = numeric_features\n",
    "    # For categorical, get onehot names\n",
    "    ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "    cat_names = ohe.get_feature_names_out(categorical_features).tolist()\n",
    "    return num_names + cat_names\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Helper: Evaluate any fitted pipeline/model (Accuracy + AUC + Runtime)\n",
    "# -----------------------------\n",
    "def evaluate_pipeline(name, pipe, Xtr, ytr, Xte, yte):\n",
    "    start = time.time()\n",
    "    pipe.fit(Xtr, ytr)\n",
    "    fit_time = time.time() - start\n",
    "\n",
    "    proba = pipe.predict_proba(Xte)[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "    return {\n",
    "        \"Stage\": name,\n",
    "        \"Accuracy\": accuracy_score(yte, pred),\n",
    "        \"AUC\": roc_auc_score(yte, proba),\n",
    "        \"Fit_time_sec\": fit_time\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# A) BASELINE (no feature selection)\n",
    "# ============================================================\n",
    "baseline = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", LogisticRegression(max_iter=4000, solver=\"liblinear\"))\n",
    "])\n",
    "\n",
    "baseline_res = evaluate_pipeline(\"Baseline (All features)\", baseline, X_train, y_train, X_test, y_test)\n",
    "print(\"\\nBASELINE:\", baseline_res)\n",
    "\n",
    "# ============================================================\n",
    "# B) FILTER METHODS\n",
    "#    We demonstrate TWO filter ideas:\n",
    "#    B1) VarianceThreshold (post-preprocessing)\n",
    "#    B2) Mutual Information (SelectKBest) (post-preprocessing)\n",
    "# ============================================================\n",
    "\n",
    "# --- B1) VarianceThreshold ---\n",
    "# Important: we apply VarianceThreshold AFTER preprocessing\n",
    "# because raw categorical strings must be encoded first.\n",
    "filter_var = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"var\", VarianceThreshold(threshold=0.01)),  # 0.01 makes sense AFTER scaling/onehot\n",
    "    (\"clf\", LogisticRegression(max_iter=4000, solver=\"liblinear\"))\n",
    "])\n",
    "\n",
    "filter_var_res = evaluate_pipeline(\"Filter: VarianceThreshold(0.01)\", filter_var, X_train, y_train, X_test, y_test)\n",
    "print(\"\\nFILTER VAR:\", filter_var_res)\n",
    "\n",
    "# --- B2) Mutual Information (MI) SelectKBest ---\n",
    "# Select top-K features by MI score with the target.\n",
    "K_MI = 40  # you can tune this (e.g., 20/40/60). Keep moderate for demo.\n",
    "filter_mi = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"mi\", SelectKBest(score_func=mutual_info_classif, k=K_MI)),\n",
    "    (\"clf\", LogisticRegression(max_iter=4000, solver=\"liblinear\"))\n",
    "])\n",
    "\n",
    "filter_mi_res = evaluate_pipeline(f\"Filter: MutualInfo top {K_MI}\", filter_mi, X_train, y_train, X_test, y_test)\n",
    "print(\"\\nFILTER MI:\", filter_mi_res)\n",
    "\n",
    "# To show which MI features were selected, we need a fitted preprocessor first:\n",
    "_ = preprocessor.fit(X_train, y_train)\n",
    "all_feature_names = get_feature_names(preprocessor, numeric_features, categorical_features)\n",
    "\n",
    "# Fit MI selector on transformed train to extract selected feature names:\n",
    "Xtr_mat = preprocessor.transform(X_train)\n",
    "mi_selector = SelectKBest(score_func=mutual_info_classif, k=K_MI).fit(Xtr_mat, y_train)\n",
    "mi_mask = mi_selector.get_support()\n",
    "mi_selected_features = [f for f, keep in zip(all_feature_names, mi_mask) if keep]\n",
    "print(\"\\nTop MI-selected features (sample):\", mi_selected_features[:15], \"...\")\n",
    "\n",
    "# ============================================================\n",
    "# C) WRAPPER METHODS\n",
    "#    C1) Sequential Forward Selection (SFS)\n",
    "#    C2) RFE (Recursive Feature Elimination)\n",
    "#\n",
    "# Note: Wrapper methods can be expensive. We'll use manageable feature sizes.\n",
    "# Strategy: start from a filtered representation (MI) to reduce dimensionality first.\n",
    "# ============================================================\n",
    "\n",
    "# Prepare filtered matrix using MI (so wrapper doesn't search over thousands of one-hot columns)\n",
    "Xtr_mi = mi_selector.transform(Xtr_mat)\n",
    "Xte_mi = mi_selector.transform(preprocessor.transform(X_test))\n",
    "\n",
    "# --- C1) SFS ---\n",
    "# SFS chooses features that maximize cross-validated ROC-AUC.\n",
    "# We'll pick a compact number (e.g., 15).\n",
    "N_SFS = min(15, Xtr_mi.shape[1])\n",
    "\n",
    "sfs_estimator = LogisticRegression(max_iter=4000, solver=\"liblinear\")\n",
    "sfs = SequentialFeatureSelector(\n",
    "    estimator=sfs_estimator,\n",
    "    n_features_to_select=N_SFS,\n",
    "    direction=\"forward\",\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "sfs.fit(Xtr_mi, y_train)\n",
    "sfs_time = time.time() - start\n",
    "\n",
    "sfs_mask = sfs.get_support()\n",
    "sfs_selected = [f for f, keep in zip(mi_selected_features, sfs_mask) if keep]\n",
    "print(\"\\nSFS selected features:\", sfs_selected)\n",
    "\n",
    "# Train a model on SFS features\n",
    "Xtr_sfs = Xtr_mi[:, sfs_mask]\n",
    "Xte_sfs = Xte_mi[:, sfs_mask]\n",
    "\n",
    "start = time.time()\n",
    "sfs_estimator.fit(Xtr_sfs, y_train)\n",
    "sfs_fit_time = time.time() - start\n",
    "\n",
    "proba = sfs_estimator.predict_proba(Xte_sfs)[:, 1]\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "wrapper_sfs_res = {\n",
    "    \"Stage\": f\"Wrapper: SFS (from MI-{K_MI} -> {N_SFS})\",\n",
    "    \"Accuracy\": accuracy_score(y_test, pred),\n",
    "    \"AUC\": roc_auc_score(y_test, proba),\n",
    "    \"Fit_time_sec\": sfs_fit_time,\n",
    "    \"Selection_time_sec\": sfs_time\n",
    "}\n",
    "print(\"\\nWRAPPER SFS:\", wrapper_sfs_res)\n",
    "\n",
    "# --- C2) RFE ---\n",
    "# RFE repeatedly fits a model and removes the weakest features.\n",
    "# We'll reduce MI-K features to N_RFE.\n",
    "N_RFE = min(20, Xtr_mi.shape[1])\n",
    "rfe_estimator = LogisticRegression(max_iter=4000, solver=\"liblinear\")\n",
    "\n",
    "rfe = RFE(estimator=rfe_estimator, n_features_to_select=N_RFE, step=1)\n",
    "\n",
    "start = time.time()\n",
    "rfe.fit(Xtr_mi, y_train)\n",
    "rfe_time = time.time() - start\n",
    "\n",
    "rfe_mask = rfe.get_support()\n",
    "rfe_selected = [f for f, keep in zip(mi_selected_features, rfe_mask) if keep]\n",
    "print(\"\\nRFE selected features:\", rfe_selected)\n",
    "\n",
    "Xtr_rfe = Xtr_mi[:, rfe_mask]\n",
    "Xte_rfe = Xte_mi[:, rfe_mask]\n",
    "\n",
    "start = time.time()\n",
    "rfe_estimator.fit(Xtr_rfe, y_train)\n",
    "rfe_fit_time = time.time() - start\n",
    "\n",
    "proba = rfe_estimator.predict_proba(Xte_rfe)[:, 1]\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "wrapper_rfe_res = {\n",
    "    \"Stage\": f\"Wrapper: RFE (from MI-{K_MI} -> {N_RFE})\",\n",
    "    \"Accuracy\": accuracy_score(y_test, pred),\n",
    "    \"AUC\": roc_auc_score(y_test, proba),\n",
    "    \"Fit_time_sec\": rfe_fit_time,\n",
    "    \"Selection_time_sec\": rfe_time\n",
    "}\n",
    "print(\"\\nWRAPPER RFE:\", wrapper_rfe_res)\n",
    "\n",
    "# ============================================================\n",
    "# D) EMBEDDED METHODS\n",
    "#    D1) L1 Logistic Regression (LASSO-like)\n",
    "#    D2) RandomForest feature importance\n",
    "#\n",
    "# We'll apply embedded methods on the WRAPPER-selected sets (SFS or RFE),\n",
    "# which is the \"Hybrid pipeline\" idea: Filter -> Wrapper -> Embedded.\n",
    "# ============================================================\n",
    "\n",
    "# --- D1) L1 Logistic Regression on SFS-selected features ---\n",
    "l1 = LogisticRegression(max_iter=6000, solver=\"liblinear\", penalty=\"l1\", C=1.0)\n",
    "\n",
    "start = time.time()\n",
    "l1.fit(Xtr_sfs, y_train)\n",
    "l1_fit_time = time.time() - start\n",
    "\n",
    "coef = l1.coef_.ravel()\n",
    "final_mask = (coef != 0)\n",
    "final_features = [f for f, keep in zip(sfs_selected, final_mask) if keep]\n",
    "\n",
    "Xte_final = Xte_sfs[:, final_mask]\n",
    "proba = l1.predict_proba(Xte_final)[:, 1]  # model expects same columns used in fit; we must refit on final columns\n",
    "\n",
    "# Refit properly on reduced set\n",
    "l1_final = LogisticRegression(max_iter=6000, solver=\"liblinear\", penalty=\"l1\", C=1.0)\n",
    "l1_final.fit(Xtr_sfs[:, final_mask], y_train)\n",
    "proba = l1_final.predict_proba(Xte_sfs[:, final_mask])[:, 1]\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "embedded_l1_res = {\n",
    "    \"Stage\": f\"Embedded: L1 Logistic (Hybrid: MI-{K_MI} -> SFS-{N_SFS} -> L1)\",\n",
    "    \"Accuracy\": accuracy_score(y_test, pred),\n",
    "    \"AUC\": roc_auc_score(y_test, proba),\n",
    "    \"Fit_time_sec\": l1_fit_time,\n",
    "    \"Final_features_count\": int(final_mask.sum())\n",
    "}\n",
    "print(\"\\nEMBEDDED L1:\", embedded_l1_res)\n",
    "print(\"Final features kept by L1:\", final_features)\n",
    "\n",
    "# --- D2) RandomForest importance on SFS-selected features ---\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "rf.fit(Xtr_sfs, y_train)\n",
    "rf_time = time.time() - start\n",
    "\n",
    "rf_importances = pd.Series(rf.feature_importances_, index=sfs_selected).sort_values(ascending=False)\n",
    "top_rf = rf_importances.head(15)\n",
    "\n",
    "# Evaluate RF directly (non-linear embedded model)\n",
    "proba = rf.predict_proba(Xte_sfs)[:, 1]\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "embedded_rf_res = {\n",
    "    \"Stage\": f\"Embedded: RandomForest (on SFS-{N_SFS})\",\n",
    "    \"Accuracy\": accuracy_score(y_test, pred),\n",
    "    \"AUC\": roc_auc_score(y_test, proba),\n",
    "    \"Fit_time_sec\": rf_time\n",
    "}\n",
    "print(\"\\nEMBEDDED RF:\", embedded_rf_res)\n",
    "print(\"\\nTop RF important features:\\n\", top_rf)\n",
    "\n",
    "# ============================================================\n",
    "# E) Summary Table (Compare everything)\n",
    "# ============================================================\n",
    "rows = []\n",
    "rows.append({**baseline_res, \"Num_features\": \"All (post-prep)\"})\n",
    "rows.append({**filter_var_res, \"Num_features\": \"Reduced (var filter)\"})\n",
    "rows.append({**filter_mi_res, \"Num_features\": f\"Top-{K_MI} (MI)\"})\n",
    "rows.append({**wrapper_sfs_res, \"Num_features\": f\"{N_SFS} (SFS)\"})\n",
    "rows.append({**wrapper_rfe_res, \"Num_features\": f\"{N_RFE} (RFE)\"})\n",
    "rows.append({**embedded_l1_res, \"Num_features\": embedded_l1_res.get(\"Final_features_count\", \"NA\")})\n",
    "rows.append({**embedded_rf_res, \"Num_features\": f\"{N_SFS} (SFS input)\"})\n",
    "\n",
    "summary = pd.DataFrame(rows)\n",
    "\n",
    "# Clean column order\n",
    "base_cols = [\"Stage\", \"Num_features\", \"Accuracy\", \"AUC\", \"Fit_time_sec\"]\n",
    "extra_cols = [c for c in summary.columns if c not in base_cols]\n",
    "summary = summary[base_cols + extra_cols]\n",
    "\n",
    "print(\"\\n\\n===== FINAL COMPARISON =====\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f9af06-c3e8-4923-aac8-7d760a4ab943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
